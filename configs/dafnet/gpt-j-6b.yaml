edit_model_name: gpt-j-6b
edit_modules: 
- transformer.h.25.mlp.fc_in
- transformer.h.25.mlp.fc_out
- transformer.h.26.mlp.fc_in
- transformer.h.26.mlp.fc_out
- transformer.h.27.mlp.fc_in
- transformer.h.27.mlp.fc_out
if_edit_bias: false
gradient_signal_batch_size: 8
aux_model:
  layer_n: 2
  inner_att_dim: 1024
  ffn_dim: 1536
  outer_att_dim: 1024
  outer_att_head_n: 2
  outer_score_att_dim: 256
  outer_score_att_head_n: 2
  norm_layer: EBN # EBN/BN/LN
training:
  aux_model_lr: 1.e-6
  init_edit_lr: 1.e-4
  edit_lr_lr: 1.e-4
  relia_lambda: 0.1
  gen_lambda: 0.1
  loc_lambda: 0.2
  max_seq_modeling: 1000 
  increase_seq_modeling_i: 3000 
  increase_seq_modeling_pace: 0.25 
  ema_loss_lambda: 0.01
  grad_clip: true
  loss_batch_size: 32 
  loss_sample_max_count: 64  
  accum_n: 1 

 
